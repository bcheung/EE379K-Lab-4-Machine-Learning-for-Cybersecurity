{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4, Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n",
    "\n",
    "The data is separated into three folders: Attack_Data_Master, Training_Data_Master, and Validation_Data_Master\n",
    "These can be found here:\n",
    "data/exercise3/Training_Data_Master\n",
    "data/exercise3/Validation_Data_Master\n",
    "data/exercise3/Attack_Data_Master\n",
    "\n",
    "All of the data in Training_Data_Master and Validation_Data_Master is normal, \n",
    "and all the data in Attack_Data_Master is malicious\n",
    "\n",
    "For the purpose of this exercise, you will ignore the predefined training/validation splits, and simply use Training_Data_Master\n",
    "and Validation_Data_Master as a single pool of normal data\n",
    "\n",
    "As mentioned, each system call trace is stored as a single file.  Treat each system call trace as a separate datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the normal system call traces (i.e., everything in Training_Data_Master and Validation_Data_Master)\n",
    "# Load all the malicious system call traces (i.e., everything in Attack_Data_Master)\n",
    "\n",
    "def load_traces(dir_str):\n",
    "    traces = []\n",
    "    directory = Path(dir_str)\n",
    "\n",
    "    for file in directory.rglob('*.txt'):\n",
    "        traces.append(file.open().readline())\n",
    "        \n",
    "    return traces\n",
    "\n",
    "train_traces = load_traces('data/exercise3/Training_Data_Master')\n",
    "valid_traces = load_traces('data/exercise3/Validation_Data_Master')\n",
    "attk_traces = load_traces('data/exercise3/Attack_Data_Master')\n",
    "\n",
    "norm_traces = train_traces + valid_traces\n",
    "mal_traces = attk_traces\n",
    "all_traces = norm_traces + mal_traces\n",
    "\n",
    "# Hint: A useful way to load this is as one or two Python lists, where each entry in the list corresponds to the text string\n",
    "#       of system calls ids; feel free to use a single list for all the data, or separate lists for malicious versus normal\n",
    "#       data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Tokenize and create a dataset where each datapoint corresponds to (normalized) counts of \n",
    "system call n-grams. Try various sizes of ngrams.\n",
    "\n",
    "Reminder: A sequence of system call IDs that looks like this:\n",
    "'6 6 63 6 42'\n",
    "\n",
    "contains the following 3-grams:\n",
    "'6 6 63'\n",
    "'6 63 6'\n",
    "'63 6 42'\n",
    "\n",
    "Note: There are a number of ways you could code this up, but if you loaded the data\n",
    "as lists of strings, you could consider using some of the feature extraction methods in \n",
    "sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the classdemo notebook for an example of doing this\n",
    "# CODE HERE\n",
    "\n",
    "# Build feature extractor\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ngram_size = 5\n",
    "count_vect = CountVectorizer(analyzer='word', ngram_range=(ngram_size, ngram_size))\n",
    "\n",
    "# Extract feature counts\n",
    "raw_cnts = count_vect.fit_transform(all_traces)\n",
    "\n",
    "# Display features\n",
    "features = count_vect.get_feature_names()\n",
    "\n",
    "# Normalize counts\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False)\n",
    "all_data = tf_transformer.fit_transform(raw_cnts)\n",
    "\n",
    "all_labels = [0] * len(norm_traces) + [1] * len(mal_traces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 50% of the data for the training set and the rest for the test set\n",
    "# CODE HERE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(all_data, all_labels, test_size=0.5, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please use Logistic Regression for this exercise\n",
    "# Feel free to experiment with the various hyperparameters available to you in sklearn\n",
    "# CODE HERE\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "classifier = SGDClassifier(loss='log', penalty='none', random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      " precision = 0.7867435158501441\n",
      "    recall = 0.773371104815864\n",
      "F1-measure = 0.78\n",
      "  accuracy = 0.948252688172043\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference on the test data and predict labels for each data point in the test data\n",
    "# CODE HERE\n",
    "\n",
    "model = classifier.fit(train_data, train_labels)\n",
    "pred_labels = model.predict(test_data)\n",
    "\n",
    "# Calculate and print the following metrics: precision, recall, f1-measure, and accuracy\n",
    "# CODE HERE\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "precision = metrics.precision_score(test_labels, pred_labels)\n",
    "recall = metrics.recall_score(test_labels, pred_labels)\n",
    "f1measure = metrics.f1_score(test_labels, pred_labels)\n",
    "accuracy = metrics.accuracy_score(test_labels, pred_labels)\n",
    "\n",
    "print('Metrics:')\n",
    "print(' precision = ' + str(precision)) # true positives / predicted positives\n",
    "print('    recall = ' + str(recall))    # true positives / actual positives\n",
    "print('F1-measure = ' + str(f1measure)) # weighted average of the precision and recall\n",
    "print('  accuracy = ' + str(accuracy))  # correctly pred / sample size\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Varying class priors\n",
    "\n",
    "Create several new test datasets where you have randomly subsampled the number of \n",
    "attack datapoints.\n",
    "\n",
    "In particular, create the following datasets:\n",
    "- 10 datasets where 25% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 50% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 75% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 90% of the attack datapoints are removed from the original test set\n",
    "- 10 datasets where 95% of the attack datapoints are removed from the original test set\n",
    "\n",
    "Report five sets of precision, recall, f1-measure, and accuracy corresponding to the following:\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 25% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 50% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 75% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 90% of attack datapoints removed\n",
    "- Average precision, recall, f1-measure, accuracy for datasets where 95% of attack datapoints removed\n",
    "\n",
    "Note: You will use the same model trained in part 1 for all of these datasets.  \n",
    "All you are varying is the class priors during the inference stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:\n",
      " precision = 0.7352941440529622\n",
      "    recall = 0.7675948239082071\n",
      "F1-measure = 0.7510223024458988\n",
      "  accuracy = 0.9528293411811075\n",
      "\n",
      "\n",
      "Metrics:\n",
      " precision = 0.6494875824522242\n",
      "    recall = 0.7811591647316893\n",
      "F1-measure = 0.7091678371606347\n",
      "  accuracy = 0.9598077891352963\n",
      "\n",
      "\n",
      "Metrics:\n",
      " precision = 0.47985206345743947\n",
      "    recall = 0.7841491598640965\n",
      "F1-measure = 0.5949015257055825\n",
      "  accuracy = 0.965728636648101\n",
      "\n",
      "\n",
      "Metrics:\n",
      " precision = 0.2745454185686528\n",
      "    recall = 0.7905918770062208\n",
      "F1-measure = 0.4061598366552195\n",
      "  accuracy = 0.9693108223096264\n",
      "\n",
      "\n",
      "Metrics:\n",
      " precision = 0.14296853468277218\n",
      "    recall = 0.7454417877444193\n",
      "F1-measure = 0.23919249579558383\n",
      "  accuracy = 0.9703012772710146\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create subsets of the test set by randomly discarding X% of points with label +1\n",
    "# CODE HERE\n",
    "\n",
    "import random\n",
    "\n",
    "percentages = [0.25, 0.50, 0.75, 0.90, 0.95]\n",
    "num_datasets = 10\n",
    "\n",
    "for percentage in percentages:\n",
    "    precision = recall = f1measure = accuracy = 0\n",
    "    for i in range(num_datasets):\n",
    "        indices_to_keep = []\n",
    "        for j in range(len(test_labels)):\n",
    "            if test_labels[j] == 0 or random.random() > percentage:\n",
    "                indices_to_keep.append(j)\n",
    "        \n",
    "        new_test_data = test_data[indices_to_keep,:]\n",
    "        new_test_labels = [test_labels[i] for i in indices_to_keep]\n",
    "        pred_labels = model.predict(new_test_data)\n",
    "\n",
    "        precision += metrics.precision_score(new_test_labels, pred_labels)\n",
    "        recall += metrics.recall_score(new_test_labels, pred_labels)\n",
    "        f1measure += metrics.f1_score(new_test_labels, pred_labels)\n",
    "        accuracy += metrics.accuracy_score(new_test_labels, pred_labels)\n",
    "\n",
    "    print('Metrics:')\n",
    "    print(' precision = ' + str(precision/num_datasets))\n",
    "    print('    recall = ' + str(recall/num_datasets))    \n",
    "    print('F1-measure = ' + str(f1measure/num_datasets)) \n",
    "    print('  accuracy = ' + str(accuracy/num_datasets))  \n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1) In Part 1, what size of ngrams gives the best performance? What are the tradeoffs as you change the size?\n",
    "\n",
    "5-grams gives the highest accuracy. As the size of ngrams increase, the recall and F1-measure decrease.\n",
    "\n",
    "2) In Part 1, how does performance change if we use simple counts as features (i.e., 1-grams) as opposed to counts of 2-grams? What does this tell you about the role of sequences in prediction for this dataset?\n",
    "\n",
    "The model performs better when using 2-grams than 1-grams. This means that certain sequences of syscalls tend to appear in malicious syscall traces rather than just trying to indentify malicious activity based on single syscalls.\n",
    "\n",
    "3) How does performance change as a function of class prior in Part 2?\n",
    "\n",
    "As the number of attack datapoints decrease, the accuracy increases. This could indicate the proportions in the training set have a smaller percentage of attack datapoints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
